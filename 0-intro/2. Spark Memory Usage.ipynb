{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f020695a-384e-4fd8-aefb-2281663b94d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b64040d1-9e02-4f65-aacf-477eb46ab897",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, count\n",
    "from pyspark.sql.functions import expr, current_timestamp, rand, randn, lit, datediff, date_sub\n",
    "from pyspark.sql.types import IntegerType\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e03b4a8-58c2-4fbb-b3e9-73acafbc1bbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Spark memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25546db3-8ff4-482b-bf4b-c18dd9d4467e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_synthetic_data_spark(num_records):\n",
    "\n",
    "   return spark.range(num_records) \\\n",
    "       .withColumn(\"customer_id\", (rand() * 9999 + 1).cast(IntegerType())) \\\n",
    "       .withColumn(\"category\", expr(\"array('Electronics', 'Clothing', 'Food', 'Books', 'Home')[cast(rand() * 5 as int)]\")) \\\n",
    "       .withColumn(\"amount\", randn() * 50 + 100) \\\n",
    "       .withColumn(\"transaction_date\", date_sub(current_timestamp(), (rand() * 365).cast(\"int\")))\n",
    "\n",
    "\n",
    "def spark_analysis(df: pyspark.sql.dataframe.DataFrame):\n",
    "    \"\"\"Perform analysis using Spark\"\"\"\n",
    "    print(f'Number of partitions: {df.rdd.getNumPartitions()}')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Calculate average amount by category\n",
    "    result = df.groupby('category').agg(\n",
    "        avg('amount').alias('mean'),\n",
    "        count('*').alias('count')\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    return result, processing_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6ae2ce5-2981-4613-9684-18f295ea3537",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----------+------------------+----------------+\n| id|customer_id|   category|            amount|transaction_date|\n+---+-----------+-----------+------------------+----------------+\n|  0|       8143|       Home| 98.13690682012054|      2024-04-18|\n|  1|       6696|Electronics| 128.2076472286725|      2024-12-26|\n|  2|       5001|Electronics|167.32913691201458|      2024-11-27|\n|  3|       6369|      Books|131.55744868111262|      2024-10-30|\n|  4|       4789|      Books|62.727262645812004|      2024-12-27|\n|  5|       7190|      Books|101.35237257201412|      2024-03-22|\n|  6|       8880|       Food| 75.03604431763159|      2024-10-03|\n|  7|       8639|       Home|  211.166402805631|      2024-07-06|\n|  8|       4540|       Food|112.22600697123166|      2025-01-23|\n|  9|       1199|       Food|20.123007696566873|      2024-08-18|\n| 10|       1099|       Food| 46.81029026244269|      2024-06-01|\n| 11|       3228|      Books|58.770660685041726|      2024-10-08|\n| 12|        724|   Clothing|  49.6926696173166|      2024-12-29|\n| 13|       2033|       Home| 108.2168848745409|      2024-05-06|\n| 14|       2109|       Home| 77.41531328052315|      2024-08-20|\n| 15|       1370|   Clothing| 71.10362824635727|      2024-10-04|\n| 16|       7123|       Home| 87.15685968483619|      2024-06-24|\n| 17|        976|       Home| 90.51633009915243|      2024-02-22|\n| 18|       6036|      Books| 98.84638091684488|      2024-06-19|\n| 19|       6551|      Books| 90.01812471080248|      2024-03-21|\n+---+-----------+-----------+------------------+----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df = generate_synthetic_data_spark(100)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "208b3958-f597-4134-84b8-2318c22a7e20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nGenerating 10,000,000 records...\nRunning Spark analysis... for 10000000 records\nNumber of partitions: 4\nSpark processing time for 10,000,000 records: 0.55 seconds\nSpark Results:\n\nGenerating 100,000,000 records...\nRunning Spark analysis... for 100000000 records\nNumber of partitions: 4\nSpark processing time for 100,000,000 records: 0.19 seconds\nSpark Results:\n\nGenerating 1,000,000,000 records...\nRunning Spark analysis... for 1000000000 records\nNumber of partitions: 4\nSpark processing time for 1,000,000,000 records: 0.58 seconds\nSpark Results:\n"
     ]
    }
   ],
   "source": [
    "sizes = [10_000_000, 100_000_000, 1_000_000_000] # 10M, 100M, 1B\n",
    "for size in sizes:\n",
    "    print(f\"\\nGenerating {size:,} records...\")\n",
    "    spark.sparkContext.setJobDescription(f\"Generating {size:,} records\")\n",
    "    print(f\"Running Spark analysis... for {size} records\")\n",
    "    spark_df = generate_synthetic_data_spark(size)\n",
    "    spark_result, spark_time = spark_analysis(spark_df)\n",
    "    print(f\"Spark processing time for {size:,} records: {spark_time:.2f} seconds\")\n",
    "    print(\"Spark Results:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d4d0066-6599-44de-a8cb-48623cf32d3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark Memory Usage",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}