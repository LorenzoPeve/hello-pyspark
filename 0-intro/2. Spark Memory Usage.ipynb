{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f020695a-384e-4fd8-aefb-2281663b94d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b64040d1-9e02-4f65-aacf-477eb46ab897",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, count\n",
    "from pyspark.sql.functions import expr, current_timestamp, rand, randn, lit, datediff, date_sub\n",
    "from pyspark.sql.types import IntegerType\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e03b4a8-58c2-4fbb-b3e9-73acafbc1bbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Spark memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25546db3-8ff4-482b-bf4b-c18dd9d4467e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_synthetic_data_spark(num_records):\n",
    "\n",
    "   return spark.range(num_records) \\\n",
    "       .withColumn(\"customer_id\", (rand() * 9999 + 1).cast(IntegerType())) \\\n",
    "       .withColumn(\"category\", expr(\"array('Electronics', 'Clothing', 'Food', 'Books', 'Home')[cast(rand() * 5 as int)]\")) \\\n",
    "       .withColumn(\"amount\", randn() * 50 + 100) \\\n",
    "       .withColumn(\"transaction_date\", date_sub(current_timestamp(), (rand() * 365).cast(\"int\")))\n",
    "\n",
    "\n",
    "def spark_analysis(df: pyspark.sql.dataframe.DataFrame):\n",
    "    \"\"\"Perform analysis using Spark\"\"\"\n",
    "    print(f'Number of partitions: {df.rdd.getNumPartitions()}')\n",
    "    result = df.groupby('category').agg(\n",
    "        avg('amount').alias('mean'),\n",
    "        count('*').alias('count')\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6ae2ce5-2981-4613-9684-18f295ea3537",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----------+-------------------+----------------+\n| id|customer_id|   category|             amount|transaction_date|\n+---+-----------+-----------+-------------------+----------------+\n|  0|       1339|   Clothing| 63.247145742748486|      2024-04-28|\n|  1|       8958|       Home|  143.3590236152635|      2024-04-20|\n|  2|       2250|Electronics| 139.92614709273062|      2024-12-13|\n|  3|       2102|   Clothing| 175.57782730903995|      2024-04-28|\n|  4|        416|      Books| 100.19031133891677|      2025-02-15|\n|  5|       7585|       Food|-26.609133469386734|      2024-09-09|\n|  6|       2999|       Food| 25.591294385776024|      2024-03-01|\n|  7|        677|       Food|  168.7696468161506|      2024-11-29|\n|  8|       7132|      Books| 101.39833832202068|      2024-04-26|\n|  9|       1830|      Books| 56.913955594264166|      2024-04-21|\n| 10|       6407|      Books| 15.117522767188902|      2025-02-03|\n| 11|       3209|   Clothing| 101.68086740885728|      2024-05-18|\n| 12|        923|       Food|  61.89780191531772|      2025-02-18|\n| 13|       4754|       Home| 14.451640596548359|      2024-10-22|\n| 14|       2739|Electronics|   77.7512669961277|      2024-07-22|\n| 15|        496|   Clothing| 177.94639594869278|      2024-10-18|\n| 16|        829|      Books| 121.16150336164185|      2024-07-16|\n| 17|        354|Electronics|  95.94076325702659|      2024-05-09|\n| 18|       3970|      Books| 133.08968302041973|      2024-12-11|\n| 19|       9225|       Home| 15.329526014546929|      2024-10-24|\n+---+-----------+-----------+-------------------+----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df = generate_synthetic_data_spark(100)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "208b3958-f597-4134-84b8-2318c22a7e20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nGenerating 10,000,000 records...\nRunning Spark analysis... for 10000000 records\nNumber of partitions: 4\nSpark Results:\n+-----------+------------------+-------+\n|   category|              mean|  count|\n+-----------+------------------+-------+\n|       Home| 99.98281523812383|1999345|\n|       Food| 99.99680065397315|1999161|\n|Electronics|100.03276193670644|2001755|\n|   Clothing| 99.97572706591164|2000398|\n|      Books| 99.99831397194481|1999341|\n+-----------+------------------+-------+\n\nSpark analysis completed in 0.533 seconds\n\nGenerating 100,000,000 records...\nRunning Spark analysis... for 100000000 records\nNumber of partitions: 4\nSpark Results:\n+-----------+------------------+--------+\n|   category|              mean|   count|\n+-----------+------------------+--------+\n|       Home|100.00026971164223|19999857|\n|       Food|100.00178138044673|20000552|\n|Electronics| 99.99485588940858|19997703|\n|   Clothing| 100.0176901641435|20007582|\n|      Books|  99.9901987618742|19994306|\n+-----------+------------------+--------+\n\nSpark analysis completed in 1.771 seconds\n\nGenerating 1,000,000,000 records...\nRunning Spark analysis... for 1000000000 records\nNumber of partitions: 4\nSpark Results:\n+-----------+------------------+---------+\n|   category|              mean|    count|\n+-----------+------------------+---------+\n|       Home| 99.99887127472734|199983389|\n|       Food|100.00127458469143|199979058|\n|Electronics|100.00161935838207|199988261|\n|   Clothing|  99.9944623533381|200015001|\n|      Books| 99.99362446386071|200034291|\n+-----------+------------------+---------+\n\nSpark analysis completed in 15.140 seconds\n"
     ]
    }
   ],
   "source": [
    "sizes = [10_000_000, 100_000_000, 1_000_000_000] # 10M, 100M, 1B\n",
    "for size in sizes:\n",
    "    print(f\"\\nGenerating {size:,} records...\")\n",
    "    spark.sparkContext.setJobDescription(f\"Generating {size:,} records\")\n",
    "    print(f\"Running Spark analysis... for {size} records\")\n",
    "    spark_df = generate_synthetic_data_spark(size)\n",
    "    spark_result = spark_analysis(spark_df)\n",
    "    print(\"Spark Results:\")\n",
    "    start = time.time()\n",
    "    spark_result.show()\n",
    "    end = time.time()\n",
    "    print(f\"Spark analysis completed in {end - start:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78edcac3-ac83-4cf5-8859-64453fdbc374",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark Memory Usage",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}