{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f020695a-384e-4fd8-aefb-2281663b94d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b64040d1-9e02-4f65-aacf-477eb46ab897",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, count\n",
    "from pyspark.sql.functions import expr, current_timestamp, rand, randn, lit, datediff, date_sub\n",
    "from pyspark.sql.types import IntegerType\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e03b4a8-58c2-4fbb-b3e9-73acafbc1bbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Spark memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25546db3-8ff4-482b-bf4b-c18dd9d4467e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_synthetic_data_spark(num_records):\n",
    "\n",
    "   return spark.range(num_records) \\\n",
    "       .withColumn(\"customer_id\", (rand() * 9999 + 1).cast(IntegerType())) \\\n",
    "       .withColumn(\"category\", expr(\"array('Electronics', 'Clothing', 'Food', 'Books', 'Home')[cast(rand() * 5 as int)]\")) \\\n",
    "       .withColumn(\"amount\", randn() * 50 + 100) \\\n",
    "       .withColumn(\"transaction_date\", date_sub(current_timestamp(), (rand() * 365).cast(\"int\")))\n",
    "\n",
    "\n",
    "def spark_analysis(df: pyspark.sql.dataframe.DataFrame):\n",
    "    \"\"\"Perform analysis using Spark\"\"\"\n",
    "    print(f'Number of partitions: {df.rdd.getNumPartitions()}')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Calculate average amount by category\n",
    "    result = df.groupby('category').agg(\n",
    "        avg('amount').alias('mean'),\n",
    "        count('*').alias('count')\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    return result, processing_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6ae2ce5-2981-4613-9684-18f295ea3537",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = generate_synthetic_data_spark(100)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "208b3958-f597-4134-84b8-2318c22a7e20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sizes = [10_000_000, 100_000_000, 1_000_000_000] # 10M, 100M, 1B\n",
    "for size in sizes:\n",
    "    print(f\"\\nGenerating {size:,} records...\")\n",
    "    spark.sparkContext.setJobDescription(f\"Generating {size:,} records\")\n",
    "    print(f\"Running Spark analysis... for {size} records\")\n",
    "    spark_df = generate_synthetic_data_spark(size)\n",
    "    spark_result, spark_time = spark_analysis(spark_df)\n",
    "    print(f\"Spark processing time for {size:,} records: {spark_time:.2f} seconds\")\n",
    "    print(\"Spark Results:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d4d0066-6599-44de-a8cb-48623cf32d3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark Memory Usage",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
